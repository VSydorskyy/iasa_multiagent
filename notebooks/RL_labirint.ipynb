{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Any, Optional, Dict\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "from matk.utils.animation import animate_frames\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R G B\n",
    "START = ((0, 255, 0), 1) \n",
    "END = ((0, 0, 100), 2)\n",
    "VOLCANO = ((255, 0, 0), 3)\n",
    "ROCK = ((96, 96, 96), 4)\n",
    "WALKER = ((255,51,153), 5)\n",
    "ESCALATOR = ((0, 255, 255), 6)\n",
    "\n",
    "def exclude_list_from_list(main_list, exclude_list):\n",
    "    return list(set(main_list) - set(exclude_list))\n",
    "\n",
    "class Labirint(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int,\n",
    "        n_rocks: int,\n",
    "        n_volcanos: int,\n",
    "        n_escalators: int,\n",
    "        escalator_len: int,\n",
    "        main_step_prob: float = 1.0,\n",
    "        start: Tuple[int,int] = (0,0),\n",
    "        end: Optional[Tuple[int,int]] = None,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.n_rocks = n_rocks\n",
    "        self.n_volcanos = n_volcanos\n",
    "        self.n_escalators = n_escalators\n",
    "        self.escalator_len = escalator_len\n",
    "        self.start = start\n",
    "        self.end = (size-1, size-1) if end is None else end\n",
    "        \n",
    "        field, states = self.create_field()\n",
    "        self.field = field\n",
    "        self.states = states\n",
    "        self.agent = Walker(start, main_step_prob=main_step_prob)\n",
    "        \n",
    "    def create_field(self):\n",
    "        field = np.zeros((self.size, self.size))\n",
    "        all_possible_coords = [(i,j) for i in range(self.size) for j in range(self.size)]\n",
    "        \n",
    "        # Place start/end\n",
    "        field[self.start] = START[1]\n",
    "        field[self.end] = END[1]\n",
    "        all_possible_coords = exclude_list_from_list(all_possible_coords, [self.start, self.end])\n",
    "        \n",
    "        # Place escalator\n",
    "        ecalator_possible_coords = [el for el in all_possible_coords if (el[1] != self.size-1) and (el[0] + self.escalator_len + 1 < self.size)]\n",
    "        escalator_places_idx = np.random.choice(list(range(len(ecalator_possible_coords))), size=self.n_escalators)\n",
    "        escalator_places = []\n",
    "        for i in escalator_places_idx:\n",
    "            field[ecalator_possible_coords[i][0]:ecalator_possible_coords[i][0]+self.escalator_len, ecalator_possible_coords[i][1]] = ESCALATOR[1]\n",
    "            escalator_places += [(ecalator_possible_coords[i][0] + j,ecalator_possible_coords[i][1]) for j in range(self.escalator_len + 1)]\n",
    "        all_possible_coords = exclude_list_from_list(all_possible_coords, list(escalator_places))\n",
    "        \n",
    "        # Place rocks\n",
    "        rock_places_idx = np.random.choice(list(range(len(all_possible_coords))), size=self.n_rocks)\n",
    "        rock_places = []\n",
    "        for i in rock_places_idx:\n",
    "            field[all_possible_coords[i]] = ROCK[1]\n",
    "            rock_places.append(all_possible_coords[i])\n",
    "        all_possible_coords = exclude_list_from_list(all_possible_coords, list(rock_places))\n",
    "        \n",
    "        # Place volkanos\n",
    "        volcano_places_idx = np.random.choice(list(range(len(all_possible_coords))), size=self.n_volcanos)\n",
    "        volcano_places = []\n",
    "        for i in volcano_places_idx:\n",
    "            field[all_possible_coords[i]] = VOLCANO[1]\n",
    "            volcano_places.append(all_possible_coords[i])\n",
    "        all_possible_coords = exclude_list_from_list(all_possible_coords, list(volcano_places))\n",
    "        \n",
    "        states = [(i,j) for i in range(self.size) for j in range(self.size)]\n",
    "        states = exclude_list_from_list(states, list(rock_places))\n",
    "        \n",
    "        \n",
    "        return field, states\n",
    "    \n",
    "    def v_print(self, input_field=None, with_agent=False, return_field=False):\n",
    "        field = self.field if input_field is None else input_field\n",
    "        \n",
    "        visualise_field = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Visualise start/end\n",
    "        visualise_field[self.start] = START[0]\n",
    "        visualise_field[self.end] = END[0]\n",
    "        \n",
    "        # Visualise escalator\n",
    "        rock_coords = np.where(field == ESCALATOR[1])\n",
    "        for x, y in zip(rock_coords[0], rock_coords[1]):\n",
    "            visualise_field[x, y] = ESCALATOR[0]\n",
    "        \n",
    "        # Visualise rocks\n",
    "        rock_coords = np.where(field == ROCK[1])\n",
    "        for x, y in zip(rock_coords[0], rock_coords[1]):\n",
    "            visualise_field[x, y] = ROCK[0]\n",
    "            \n",
    "        # Visualise volcano\n",
    "        volcano_coords = np.where(field == VOLCANO[1])\n",
    "        for x, y in zip(volcano_coords[0], volcano_coords[1]):\n",
    "            visualise_field[x, y] = VOLCANO[0]\n",
    "            \n",
    "        # Visualise walker\n",
    "        if with_agent:\n",
    "            visualise_field[self.agent.coord[0],self.agent.coord[1]] = WALKER[0]\n",
    "        \n",
    "        if return_field:\n",
    "            return visualise_field\n",
    "        else:\n",
    "            plt.imshow(visualise_field)\n",
    "            plt.plot()\n",
    "        \n",
    "    def get_possible_actions(self, coords):\n",
    "        if self.field[coords[0], coords[1]] == ESCALATOR[1]:\n",
    "            return [\"down\"]\n",
    "        \n",
    "        possible_actions = []\n",
    "        if coords[1] - 1 >= 0 and self.field[coords[0], coords[1] - 1] != ROCK[1]:\n",
    "            possible_actions.append(\"left\")\n",
    "        if coords[1] + 1 < self.size and self.field[coords[0], coords[1] + 1] != ROCK[1]:\n",
    "            possible_actions.append(\"right\")\n",
    "        if coords[0] - 1 >= 0 and self.field[coords[0] - 1, coords[1]] != ROCK[1]:\n",
    "            possible_actions.append(\"up\")\n",
    "        if coords[0] + 1 < self.size and self.field[coords[0] + 1, coords[1]] != ROCK[1]:\n",
    "            possible_actions.append(\"down\")\n",
    "            \n",
    "        return possible_actions\n",
    "    \n",
    "    def get_reward(self, coords):\n",
    "        if self.field[coords[0], coords[1]] in [0,START[1], ESCALATOR[1]]:\n",
    "            return -1\n",
    "        if self.field[coords[0], coords[1]] == VOLCANO[1]:\n",
    "            return -50\n",
    "        if self.field[coords[0], coords[1]] == END[1]:\n",
    "            return 100\n",
    "        \n",
    "    def get_endgame(self, coords):\n",
    "        if self.field[coords[0], coords[1]] in [VOLCANO[1],END[1]]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_success(self, coords):\n",
    "        if self.field[coords[0], coords[1]] == END[1]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_escalator(self, coords):\n",
    "        if self.field[coords[0], coords[1]] == ESCALATOR[1]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "class Walker(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        coord: Tuple[int,int],\n",
    "        main_step_prob: float = 1.0\n",
    "    ):\n",
    "        self.start_coord = coord\n",
    "        self.main_step_prob = main_step_prob\n",
    "        self.coord = list(coord)\n",
    "        \n",
    "    def random_step(self, real_step, possible_actions):\n",
    "        if len(possible_actions) == 1:\n",
    "            return real_step\n",
    "        \n",
    "        if np.random.binomial(n=2, p=self.main_step_prob):\n",
    "            return real_step\n",
    "        else:\n",
    "            return np.random.choice(exclude_list_from_list(possible_actions, [real_step]))\n",
    "        \n",
    "    def get_coord_from_action(self, action, cur_coord):\n",
    "        x, y = cur_coord\n",
    "        if action == \"left\":\n",
    "            y -= 1\n",
    "        elif action == \"right\":\n",
    "            y += 1\n",
    "        elif action == \"up\":\n",
    "            x -= 1\n",
    "        elif action == \"down\":\n",
    "            x += 1\n",
    "        return x, y\n",
    "        \n",
    "    def step(self, direction, possible_actions, do_step=True):\n",
    "        if direction not in possible_actions:\n",
    "            raise RuntimeError(\"direction should be in possible_actions\")\n",
    "        \n",
    "        direction = self.random_step(direction, possible_actions)\n",
    "        new_x, new_y = self.get_coord_from_action(direction, tuple(self.coord))\n",
    "        self.coord[0] = new_x\n",
    "        self.coord[1] = new_y\n",
    "    \n",
    "    def reset_coord(self):\n",
    "        self.coord = list(self.start_coord)\n",
    "            \n",
    "class ValueItteration(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        environment: object,\n",
    "        environment_config: Dict[str, Any],\n",
    "        gamma: float,\n",
    "        epsilon: float\n",
    "    ):\n",
    "        self.environment = environment(**environment_config)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.policy = {}\n",
    "        for s in self.environment.states:\n",
    "            self.policy[s] = np.random.choice(self.environment.get_possible_actions(s))\n",
    "        self.hashed_policies = [deepcopy(self.policy)]\n",
    "            \n",
    "        self.v = {}\n",
    "        for s in self.environment.states:\n",
    "            self.v[s] = self.environment.get_reward(s)\n",
    "            \n",
    "    def run_policy(self, max_iter=1_000, input_policy=None, verbose=True):\n",
    "        policy = self.policy if input_policy is None else input_policy\n",
    "        self.environment.agent.reset_coord()\n",
    "        terminated = False\n",
    "        it = 0\n",
    "        n_escalators_steps = 0\n",
    "        all_fields = [self.environment.v_print(with_agent=True, return_field=True)]\n",
    "        while not terminated and it < max_iter:\n",
    "            state = tuple(self.environment.agent.coord)\n",
    "            action = policy[state]\n",
    "            self.environment.agent.step(action, self.environment.get_possible_actions(state))\n",
    "            all_fields.append(self.environment.v_print(with_agent=True, return_field=True))\n",
    "            terminated = self.environment.get_endgame(self.environment.agent.coord)\n",
    "            n_escalators_steps += int(self.environment.get_escalator(self.agent.coord))\n",
    "            if terminated and verbose: \n",
    "                print(\"End game\")\n",
    "            it += 1\n",
    "        success = self.environment.get_success(self.environment.agent.coord)\n",
    "        return all_fields, success, n_escalators_steps\n",
    "    \n",
    "    def run_algorithm(self):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            biggest_change = 0\n",
    "            for s in self.environment.states:            \n",
    "                if not self.environment.get_endgame(s):\n",
    "\n",
    "                    possible_actions = self.environment.get_possible_actions(s)\n",
    "                    if len(possible_actions) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    old_v = self.v[s]\n",
    "                    new_v = 0\n",
    "\n",
    "                    for a in possible_actions:\n",
    "                        value = self.v[self.environment.agent.get_coord_from_action(a, s)]\n",
    "                        \n",
    "                        if len(possible_actions) == 1:\n",
    "                            main_prob = 1.0\n",
    "                            additional_prob = 0\n",
    "                        else:\n",
    "                            main_prob = self.environment.agent.main_step_prob\n",
    "                            additional_prob = (1 - self.environment.agent.main_step_prob) / (len(possible_actions) - 1)\n",
    "                            \n",
    "                        additional_value = sum([self.v[self.environment.agent.get_coord_from_action(a_r, s)] for a_r in possible_actions if a_r != a])\n",
    "                        \n",
    "                        v = self.environment.get_reward(s) + (self.gamma * (value*main_prob + additional_prob*additional_value))\n",
    "\n",
    "                        if v > new_v:\n",
    "                            new_v = v\n",
    "                            self.policy[s] = a\n",
    "\n",
    "                    self.v[s] = new_v\n",
    "                    biggest_change = max(biggest_change, np.abs(old_v - self.v[s]))\n",
    "\n",
    "            self.hashed_policies.append(deepcopy(self.policy))\n",
    "            if biggest_change < self.epsilon:\n",
    "                break\n",
    "            iteration += 1\n",
    "            \n",
    "        print(f\"Converged in {iteration} iterations\")\n",
    "        \n",
    "class QLearning(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        environment: object,\n",
    "        environment_config: Dict[str, Any],\n",
    "        gamma: float,\n",
    "        epsilon: float,\n",
    "        lr: float,\n",
    "        lr_shed: Optional[float] = None,\n",
    "        epsilon_shed: Optional[float] = None\n",
    "    ):\n",
    "        self.environment = environment(**environment_config)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.lr_shed = lr_shed\n",
    "        self.epsilon_shed = epsilon_shed\n",
    "        \n",
    "        self.q_table = {}\n",
    "        for s in self.environment.states:\n",
    "            self.q_table[s] = {k:0 for k in self.environment.get_possible_actions(s)}\n",
    "            \n",
    "        self.hashed_runs = []\n",
    "            \n",
    "    def run_policy(self, max_iter=1_000, verbose=True):\n",
    "        self.environment.agent.reset_coord()\n",
    "        terminated = False\n",
    "        it = 0\n",
    "        n_escalators_steps = 0\n",
    "        all_fields = [self.environment.v_print(with_agent=True, return_field=True)]\n",
    "        while not terminated and it < max_iter:\n",
    "            state = tuple(self.environment.agent.coord)\n",
    "            actions = self.q_table[state]\n",
    "            action = max(actions, key=actions.get)\n",
    "            self.environment.agent.step(action, self.environment.get_possible_actions(state))\n",
    "            all_fields.append(self.environment.v_print(with_agent=True, return_field=True))\n",
    "            terminated = self.environment.get_endgame(self.environment.agent.coord)\n",
    "            n_escalators_steps += int(self.environment.get_escalator(self.agent.coord))\n",
    "            if terminated and verbose:\n",
    "                print(\"End game\")\n",
    "            it += 1\n",
    "        success = self.environment.get_success(self.environment.agent.coord)\n",
    "        return all_fields, success, n_escalators_steps\n",
    "    \n",
    "    def run_algorithm(self, n_steps):\n",
    "        lr = self.lr\n",
    "        epsilon = self.epsilon\n",
    "        self.hashed_runs = []\n",
    "        for step in tqdm(range(n_steps)):\n",
    "            self.environment.agent.reset_coord()\n",
    "            terminated = False\n",
    "            all_fields = [self.environment.v_print(with_agent=True, return_field=True)]\n",
    "            while not terminated:\n",
    "\n",
    "                state = tuple(self.environment.agent.coord)\n",
    "                state_actions = self.q_table[state]\n",
    "\n",
    "                if np.random.binomial(n=2, p=epsilon):\n",
    "                    # Explore\n",
    "                    action = np.random.choice(list(state_actions.keys()))\n",
    "                else:\n",
    "                    # Exploit\n",
    "                    action = max(state_actions, key=state_actions.get)\n",
    "\n",
    "                new_state = self.environment.agent.get_coord_from_action(action, state)\n",
    "                reward = self.environment.get_reward(new_state)\n",
    "\n",
    "                self.q_table[state][action] = lr * (reward + self.gamma*max(self.q_table[new_state].values())) + (1-lr)*self.q_table[state][action]\n",
    "\n",
    "                self.environment.agent.step(action, self.environment.get_possible_actions(state))\n",
    "                all_fields.append(self.environment.v_print(with_agent=True, return_field=True))\n",
    "                terminated = self.environment.get_endgame(self.environment.agent.coord)\n",
    "                \n",
    "            success = self.environment.get_success(self.environment.agent.coord)\n",
    "            self.hashed_runs.append((all_fields, success))\n",
    "            if self.lr_shed is not None:\n",
    "                lr = lr * self.lr_shed\n",
    "            if self.epsilon_shed is not None:\n",
    "                epsilon = epsilon * self.epsilon_shed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iter = ValueItteration(\n",
    "    Labirint,\n",
    "    {\n",
    "        \"size\":10,\n",
    "        \"n_rocks\":10,\n",
    "        \"n_volcanos\":10,\n",
    "        \"n_escalators\":2,\n",
    "        \"escalator_len\":5,\n",
    "        \"main_step_prob\":0.85\n",
    "    },\n",
    "    epsilon=0.005,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "value_iter.environment.v_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iter.run_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions, success = value_iter.run_policy()\n",
    "print(\"Finished\" if success else \"Not Finished\")\n",
    "ani = animate_frames(all_actions, figsize=(10,10), interval=200)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_convergance = [value_iter.run_policy(input_policy=p, verbose=False)[1] for p in value_iter.hashed_policies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(success_convergance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn = QLearning(\n",
    "    Labirint,\n",
    "    {\n",
    "        \"size\":10,\n",
    "        \"n_rocks\":10,\n",
    "        \"n_volcanos\":10,\n",
    "        \"n_escalators\":2,\n",
    "        \"escalator_len\":5,\n",
    "        \"main_step_prob\":0.9\n",
    "    },\n",
    "    epsilon=1.0,\n",
    "    gamma=0.9,\n",
    "    lr=1.0,\n",
    "    lr_shed=0.99,\n",
    "    epsilon_shed=0.99\n",
    ")\n",
    "\n",
    "q_learn.environment.v_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn.run_algorithm(n_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions, success = q_learn.run_policy()\n",
    "print(\"Finished\" if success else \"Not Finished\")\n",
    "ani = animate_frames(all_actions, figsize=(10,10), interval=200)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_convergance = [el[1] for el in q_learn.hashed_runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(success_convergance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
